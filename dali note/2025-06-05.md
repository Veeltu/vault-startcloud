start 8:30
[] vyos metrics - building local infra obs
12:30 / 4h


[[Sprawdz cofig otel collector]]
[[Co pokazuje polecenie netstat -tulnp]]
[[diagnostics otel-pod microk8s]]

[[Co to jest Gateway API w Kubernetesie]]
[[traefik]]
***
### todo:
- ingres dla otel - fix
- 

req:
ingres 
cert-namanger
helm
CRD,gateway api - manual
metalLB
env-s for

```
sudo microk8s kubectl port-forward svc/otel-collector 4317:4317 -n network

netstat -tulnp | grep 4317

#sprawdz IP na podach
sudo microk8s kubectl get pods -o wide -n network

#sprawdz zawartosc config map
microk8s kubectl get configmap otel-collector-config -n network -o yaml

 # pelna definicja / manifest w yaml
 sudo microk8s kubectl get pod otel-collector-fc59c468-lnkqz -n network -o yaml
 
#wszyskie secrety
sudo microk8s kubectl get secrets -n network

# przed dodawanie do istniejacego networku
terraform import kubernetes_namespace.my-network my-network

# logi po klapie na podzie
microk8s kubectl logs -n my-network otel-collector-666d4f7b64-w6b6x

#delete deploymnet
sudo microk8s kubectl delete deployment otel-collector -n my-network

# jak error ze helm traefik istnieje juz
microk8s helm uninstall traefik --namespace traefik
terragrunt destroy -target=helm_release.traefik

# bylo widac ze external IP nie byl przydzielony
 microk8s kubectl get all -n traefik

#status
microk8s helm status traefik -n traefik

#delete deploymeny 
microk8s kubectl delete deployment otel-collector -n otel-collector

#get all in namespace
microk8s kubectl get all -n network

#describe
microk8s kubectl describe pod otel-collector-6d545648f4-k8wrc -n network


```

sesja diagnostyczna z zew podem
```
# Uruchom pod testowy
microk8s kubectl run -n network test-pod --rm -it --image=nicolaka/netshoot -- /bin/sh

# Wewnątrz poda testowego:
nc -vz 10.1.19.151 4317
nc -vz 10.1.19.151 4318
nc -vz 10.1.19.151 8889

# Jeśli porty są otwarte, zobaczysz:
# Connection to 10.1.19.151 port 4317 [tcp/*] succeeded!

```

```
2025-06-11T11:12:56.321Z    
warn    
internal/transaction.go:150    
Failed to scrape Prometheus endpoint    
{"resource": {"service.instance.id": "4e59094a-ce81-4262-92ed-aeba10468228", "service.name": "otelcol-contrib", "service.version": "0.128.0"}, 
"otelcol.component.id": "prometheus/vyos", "otelcol.component.kind": "receiver", "otelcol.signal": "metrics", "scrape_timestamp": 1749640376312, 
"target_labels": "{__name__=\"up\", instance=\"er-2516c83242e043ea93c56fc04589075f.doe.stable.pndrs.de\", job=\"snmp\"}"}
```